# Getting Started with Apache Spark and Core API's

> Apache Spark is an **in-memory cluster computing** designed to handle a wide range of big data workloads

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdZ3OMAMb9i7U2qFYqDkCE4rZ6D5raA88eAhg_5t-pfI5gNb9HJv32u7cd3aGMG7vJ9wZM8iopA_JxU-D86EWJAsniC6M6NFU6v8GyLnqRS9EBOmEqel8JDMyQt6ArSHKOAIzPouPKF_BXmFOaohJBaDDu7?key=Lcjgu0sLjm8U8i3A_14gRg)

1. Data Integration and ETL
2. High Performance Batch Computation
3. Machine Learning Analytics
4. Real-time stream processing
5. Graph Processing

**Important Points**

* Apache Spark is natively written using Scala

## PySpark

> PySpark is an API for Apache Spark

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcrpQsHMy7n7Qh9lID-nEVwkdLvDxzNxv-a8dXwBID6atFfOGCmKTiFHURM2EaPC_lzxELqXCbI7LE1XSAIbEzzKXGwBOZv-Iid7XyBpfNogoMWb0dR4yW7bF8q6S24MiTnjjXHZ-kEicSEtvfTaOmkGe4?key=uvmlVet7-pBAx-jz0PuzLA)



* The PySpark communicates with SPark using Py4J API

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeQihXBZ1Ujw8VtOJLhSL4D0X6Ds9JPQTeDs8uCKNE5SKusC-4y5Qmfhitw8wb9i0Nc_q9RDd9r1dQRzHfnccVMeXVitnlDX98daFW6G8FqOqXzZLE3sM2qUTipIednICGLR8KP5b5hoWqxX1TlwlMPyYQ?key=uvmlVet7-pBAx-jz0PuzLA)

## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdzE1o9o-X5vThyskh9FvDAyfiojlsF34azy6dBePxgpiiwfxYLk8zgpV1nH_tOUSBdqmFGVOVbsmwHpmlgHzgQOU7DUL4mgMzJfIR4xkAcvBdJMAyZ-vDbw030rCQbaRKKPHs6ea-AbfoFXJA4rUY0Nl0?key=uvmlVet7-pBAx-jz0PuzLA)

## Spark Interactive Shell

Spark provides command line interface to write and execute spark programs

1. Spark Shell (Scala)
2. PySpark Shell (Python)

## API's supported

There are two levels of API

1. Low Level API
2. High Level API



## RDD's 

> RDD's are the building blocks of any spark application

## Partitions

RDD is a collection of objects that is partitioned and distributed across nodes in a cluster

A partition is a logical chunk of large dataset

## RDD Creation

There are two popular ways to create RDD

1. Create an RDD from Collection
2. Create an RDD from external source

**Notes** : All the methods to create RDD is present inside SparkContext (sc)



## RDD Operations

Once an RDD is created we can perform two types of operations

1. Transformation
2. Actions

![img](https://camo.githubusercontent.com/61f0ff9773447ae3e96d6f5953f24dc74d609ce6a0c21095b796e5683697701f/68747470733a2f2f6c68372d72742e676f6f676c6575736572636f6e74656e742e636f6d2f646f63737a2f41445f346e5863384a4c2d5a53565f4f6d7542734f775a653654734259644978467a374f34524f446c75624e554a47353631415542316c63515230624d48324b2d7065584961455137506d4834476f654a6d7a7944564d43566632413243546b457743764f674a5258666a34324e59774744417a3562556c55457a57576c3776376b6552783836355a51782d68396a59584a396a75516738617271744c59303f6b65793d75766d6c566574372d704241782d6a7a3050757a4c41)

### Transformations

* Transformation creates a new RDD
* E.g .map(func), .filter(func)

### Actions

* Actions are the final operations on the RDD
