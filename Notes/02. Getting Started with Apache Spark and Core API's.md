# Getting Started with Apache Spark and Core API's

> Apache Spark is an **in-memory cluster computing** designed to handle a wide range of big data workloads

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdZ3OMAMb9i7U2qFYqDkCE4rZ6D5raA88eAhg_5t-pfI5gNb9HJv32u7cd3aGMG7vJ9wZM8iopA_JxU-D86EWJAsniC6M6NFU6v8GyLnqRS9EBOmEqel8JDMyQt6ArSHKOAIzPouPKF_BXmFOaohJBaDDu7?key=Lcjgu0sLjm8U8i3A_14gRg)

1. Data Integration and ETL
2. High Performance Batch Computation
3. Machine Learning Analytics
4. Real-time stream processing
5. Graph Processing

**Important Points**

* Apache Spark is natively written using Scala

## PySpark

> PySpark is an API for Apache Spark

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcrpQsHMy7n7Qh9lID-nEVwkdLvDxzNxv-a8dXwBID6atFfOGCmKTiFHURM2EaPC_lzxELqXCbI7LE1XSAIbEzzKXGwBOZv-Iid7XyBpfNogoMWb0dR4yW7bF8q6S24MiTnjjXHZ-kEicSEtvfTaOmkGe4?key=uvmlVet7-pBAx-jz0PuzLA)



* The PySpark communicates with SPark using Py4J API

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeQihXBZ1Ujw8VtOJLhSL4D0X6Ds9JPQTeDs8uCKNE5SKusC-4y5Qmfhitw8wb9i0Nc_q9RDd9r1dQRzHfnccVMeXVitnlDX98daFW6G8FqOqXzZLE3sM2qUTipIednICGLR8KP5b5hoWqxX1TlwlMPyYQ?key=uvmlVet7-pBAx-jz0PuzLA)

## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdzE1o9o-X5vThyskh9FvDAyfiojlsF34azy6dBePxgpiiwfxYLk8zgpV1nH_tOUSBdqmFGVOVbsmwHpmlgHzgQOU7DUL4mgMzJfIR4xkAcvBdJMAyZ-vDbw030rCQbaRKKPHs6ea-AbfoFXJA4rUY0Nl0?key=uvmlVet7-pBAx-jz0PuzLA)

## Spark Interactive Shell

Spark provides command line interface to write and execute spark programs

1. Spark Shell (Scala)
2. PySpark Shell (Python)

## API's supported

There are two levels of API

1. Low Level API
2. High Level API



## RDD's 

> RDD's are the building blocks of any spark application

## Partitions

RDD is a collection of objects that is partitioned and distributed across nodes in a cluster

A partition is a logical chunk of large dataset

## RDD Creation

There are two popular ways to create RDD

1. Create an RDD from Collection
2. Create an RDD from external source

**Notes** : All the methods to create RDD is present inside SparkContext (sc)
