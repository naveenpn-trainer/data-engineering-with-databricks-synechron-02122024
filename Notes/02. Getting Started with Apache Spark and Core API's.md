# Getting Started with Apache Spark and Core API's

> Apache Spark is an in-memory cluster computing designed to handle a wide range of big data workloads

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdZ3OMAMb9i7U2qFYqDkCE4rZ6D5raA88eAhg_5t-pfI5gNb9HJv32u7cd3aGMG7vJ9wZM8iopA_JxU-D86EWJAsniC6M6NFU6v8GyLnqRS9EBOmEqel8JDMyQt6ArSHKOAIzPouPKF_BXmFOaohJBaDDu7?key=Lcjgu0sLjm8U8i3A_14gRg)

1. Data Integration and ETL
2. High Performance Batch Computation
3. Machine Learning Analytics
4. Real-time stream processing

**Important Points**

* Apache Spark is natively written using Scala

## PySpark

> PySpark is an API for Apache Spark

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcrpQsHMy7n7Qh9lID-nEVwkdLvDxzNxv-a8dXwBID6atFfOGCmKTiFHURM2EaPC_lzxELqXCbI7LE1XSAIbEzzKXGwBOZv-Iid7XyBpfNogoMWb0dR4yW7bF8q6S24MiTnjjXHZ-kEicSEtvfTaOmkGe4?key=uvmlVet7-pBAx-jz0PuzLA)



* The PySpark communicates with SPark using Py4J API

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeQihXBZ1Ujw8VtOJLhSL4D0X6Ds9JPQTeDs8uCKNE5SKusC-4y5Qmfhitw8wb9i0Nc_q9RDd9r1dQRzHfnccVMeXVitnlDX98daFW6G8FqOqXzZLE3sM2qUTipIednICGLR8KP5b5hoWqxX1TlwlMPyYQ?key=uvmlVet7-pBAx-jz0PuzLA)

