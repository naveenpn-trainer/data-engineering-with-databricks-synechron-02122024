{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e465d68b-3572-4c66-9a43-44982822483b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I just love **bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fca8c2a9-fb56-4206-b4ae-a311a3c58b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ea7b1d-4b32-4d64-8d82-999eb7233557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: pyspark.sql.session.SparkSession"
     ]
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6053052-f424-452a-8584-a2a2a8c17bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# RDD Creation\n",
    "\n",
    "> RDD are the fundamental data structure of Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd3adfd-d422-4581-8379-d8f2526cba41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create RDD from Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d18059-cfe0-4f71-81c1-6a76b61befc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: pyspark.rdd.RDD"
     ]
    }
   ],
   "source": [
    "L = list(range(1,101))\n",
    "numbers_rdd = sc.parallelize(L)\n",
    "type(numbers_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "006c93fc-71cf-4191-8ab9-bdaa0ec08f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create RDD from External source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b98673-9d6f-42dd-b9ac-f17fb879bcac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2638977996738694>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m users_rdd \u001B[38;5;241m=\u001B[39m sc\u001B[38;5;241m.\u001B[39mtextFile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/synechron/users/csv_format/users_001.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 2\u001B[0m df \u001B[38;5;241m=\u001B[39m users_rdd\u001B[38;5;241m.\u001B[39mtoDF([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgen\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdesignation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[1;32m      3\u001B[0m df\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m4\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:113\u001B[0m, in \u001B[0;36m_monkey_patch_RDD.<locals>.toDF\u001B[0;34m(self, schema, sampleRatio)\u001B[0m\n",
       "\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n",
       "\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sampleRatio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n",
       "\u001B[1;32m     80\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001B[39;00m\n",
       "\u001B[1;32m     82\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03m    +---+\u001B[39;00m\n",
       "\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampleRatio\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n",
       "\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n",
       "\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n",
       "\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n",
       "\u001B[1;32m   1215\u001B[0m     )\n",
       "\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n",
       "\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1264\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n",
       "\u001B[1;32m   1262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1263\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, RDD):\n",
       "\u001B[0;32m-> 1264\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromRDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1265\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1266\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:834\u001B[0m, in \u001B[0;36mSparkSession._createFromRDD\u001B[0;34m(self, rdd, schema, samplingRatio)\u001B[0m\n",
       "\u001B[1;32m    830\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    831\u001B[0m \u001B[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001B[39;00m\n",
       "\u001B[1;32m    832\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    833\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
       "\u001B[0;32m--> 834\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    835\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n",
       "\u001B[1;32m    836\u001B[0m     tupled_rdd \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(converter)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:787\u001B[0m, in \u001B[0;36mSparkSession._inferSchema\u001B[0;34m(self, rdd, samplingRatio, names)\u001B[0m\n",
       "\u001B[1;32m    785\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n",
       "\u001B[1;32m    786\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m samplingRatio \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m--> 787\u001B[0m     schema \u001B[38;5;241m=\u001B[39m \u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    788\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfirst\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    789\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    790\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    791\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    793\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n",
       "\u001B[1;32m    794\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m rdd\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m100\u001B[39m)[\u001B[38;5;241m1\u001B[39m:]:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1441\u001B[0m, in \u001B[0;36m_infer_schema\u001B[0;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001B[0m\n",
       "\u001B[1;32m   1438\u001B[0m     items \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(row\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems())\n",
       "\u001B[1;32m   1440\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan not infer schema for type: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(row))\n",
       "\u001B[1;32m   1443\u001B[0m fields \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[1;32m   1444\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m items:\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: Can not infer schema for type: <class 'str'>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2638977996738694>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m users_rdd \u001B[38;5;241m=\u001B[39m sc\u001B[38;5;241m.\u001B[39mtextFile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/synechron/users/csv_format/users_001.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m df \u001B[38;5;241m=\u001B[39m users_rdd\u001B[38;5;241m.\u001B[39mtoDF([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgen\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdesignation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      3\u001B[0m df\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m4\u001B[39m)\u001B[38;5;241m.\u001B[39mdisplay()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:113\u001B[0m, in \u001B[0;36m_monkey_patch_RDD.<locals>.toDF\u001B[0;34m(self, schema, sampleRatio)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sampleRatio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03m    +---+\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampleRatio\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1215\u001B[0m     )\n\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1264\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1263\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, RDD):\n\u001B[0;32m-> 1264\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromRDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1265\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1266\u001B[0m         rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromLocal(\u001B[38;5;28mmap\u001B[39m(prepare, data), schema)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:834\u001B[0m, in \u001B[0;36mSparkSession._createFromRDD\u001B[0;34m(self, rdd, schema, samplingRatio)\u001B[0m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    831\u001B[0m \u001B[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    833\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 834\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    835\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m    836\u001B[0m     tupled_rdd \u001B[38;5;241m=\u001B[39m rdd\u001B[38;5;241m.\u001B[39mmap(converter)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:787\u001B[0m, in \u001B[0;36mSparkSession._inferSchema\u001B[0;34m(self, rdd, samplingRatio, names)\u001B[0m\n\u001B[1;32m    785\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[1;32m    786\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m samplingRatio \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 787\u001B[0m     schema \u001B[38;5;241m=\u001B[39m \u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfirst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    793\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[1;32m    794\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m rdd\u001B[38;5;241m.\u001B[39mtake(\u001B[38;5;241m100\u001B[39m)[\u001B[38;5;241m1\u001B[39m:]:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1441\u001B[0m, in \u001B[0;36m_infer_schema\u001B[0;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m   1438\u001B[0m     items \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msorted\u001B[39m(row\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems())\n\u001B[1;32m   1440\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan not infer schema for type: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(row))\n\u001B[1;32m   1443\u001B[0m fields \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   1444\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m items:\n\n\u001B[0;31mTypeError\u001B[0m: Can not infer schema for type: <class 'str'>",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: Can not infer schema for type: <class 'str'>",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "users_rdd = sc.textFile(\"dbfs:/FileStore/synechron/users/csv_format/users_001.csv\")\n",
    "df = users_rdd.toDF([\"id\",\"age\",\"gen\",\"designation\",\"salary\"])\n",
    "df.limit(4).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3a56e84-f1ba-43a6-90fe-7479cf07e4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "535af27c-6c20-4912-a17a-bd90ea9942c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20267f85-9a25-4fbd-9104-00f78b9199e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The .map(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc35a179-4050-4006-a0e4-f26838c655dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: [3, 6, 9, 12, 15]"
     ]
    }
   ],
   "source": [
    "rdd_01 = numbers_rdd.map(lambda e:e*3)\n",
    "rdd_01.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7923e8d-4f34-4728-a5c5-58b4a1c68174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cfd5314-8b39-4f13-b658-6d1b521048d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The .filter(predFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69f0448-4c56-4b13-87d9-a1c763be79fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: [15,\n 30,\n 45,\n 60,\n 75,\n 90,\n 105,\n 120,\n 135,\n 150,\n 165,\n 180,\n 195,\n 210,\n 225,\n 240,\n 255,\n 270,\n 285,\n 300]"
     ]
    }
   ],
   "source": [
    "rdd_02 = rdd_01.filter(lambda e:e%15==0)\n",
    "rdd_02.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "143bd620-2401-4298-acdf-ad51745b78d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f9a68c5-3400-45a6-b5be-776f02b89de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The .take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ed09ef-251c-4769-918f-34e4676b11af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n6\n9\n12\n15\n"
     ]
    }
   ],
   "source": [
    "for record in rdd_01.take(5):\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aa380e2-a534-4fc3-89a1-08f9ba893362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05b5dca-7ad1-4870-ae8e-78aa83559934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,age,gen,designation,salary\n1,26,M,Technician,85711\n2,53,F,Other,94043\n3,23,M,Writer,32067\n4,26,M,Technician,43537\n5,33,F,Other,15213\n6,42,M,Cheif Executive Officer,98101\n7,57,M,Administrator,91344\n8,36,M,Administrator,05201\n9,29,M,Student,01002\n10,53,M,Lawyer,90703\n11,39,F,Other,30329\n12,28,F,Other,06405\n13,47,M,Educator,29206\n14,45,M,Scientist,55106\n15,49,F,Educator,97301\n16,21,M,Entertainment,10309\n17,30,M,Programmer,06355\n18,35,F,Other,37212\n19,40,M,Librarian,02138\n20,42,F,Homemaker,95660\n21,26,M,Writer,30068\n22,25,M,Writer,40206\n23,30,F,Artist,48197\n24,21,F,Artist,94533\n25,39,M,Engineer,55107\n26,49,M,Engineer,21044\n27,40,F,Librarian,30030\n28,32,M,Writer,55369\n29,41,M,Programmer,94043\n30,7,M,Student,55436\n31,24,M,Artist,10003\n32,28,F,Student,78741\n33,23,M,Student,27510\n34,38,F,Administrator,42141\n35,20,F,Homemaker,42459\n36,19,F,Student,93117\n37,23,M,Student,55105\n38,28,F,Other,54467\n39,41,M,Entertainment,01040\n40,38,M,Scientist,27514\n41,33,M,Engineer,80525\n42,30,M,Administrator,17870\n43,29,F,Librarian,20854\n44,26,M,Technician,46260\n45,29,M,Programmer,50233\n46,27,F,marketing,46538\n47,53,M,marketing,07102\n48,45,M,Administrator,12550\n49,23,F,Student,76111\n50,21,M,Writer,52245\n51,28,M,Educator,16509\n52,18,F,Student,55105\n53,26,M,Programmer,55414\n54,22,M,Cheif Executive Officer,66315\n55,37,M,Programmer,01331\n56,25,M,Librarian,46260\n57,16,M,Cheif Cheif Executive Officer Officer,84010\n58,27,M,Programmer,52246\n59,49,M,Educator,08403\n60,50,M,healthcare,06472\n61,36,M,Engineer,30040\n62,27,F,Administrator,97214\n63,31,M,marketing,75240\n64,32,M,Educator,43202\n65,51,F,Educator,48118\n66,23,M,Student,80521\n67,17,M,Student,60402\n68,19,M,Student,22904\n69,24,M,Engineer,55337\n70,27,M,Engineer,60067\n71,39,M,Scientist,98034\n72,48,F,Administrator,73034\n73,24,M,Student,41850\n74,39,M,Scientist,41850\n75,24,M,Entertainment,08816\n76,20,M,Student,02215\n77,30,M,Technician,29379\n78,26,M,Administrator,61801\n79,39,F,Administrator,03755\n80,34,F,Administrator,52241\n81,21,M,Student,21218\n82,50,M,Programmer,22902\n83,40,M,Other,44133\n84,32,M,Cheif Executive Officer,55369\n85,51,M,Educator,20003\n86,26,M,Administrator,46005\n87,47,M,Administrator,89503\n88,49,F,Librarian,11701\n89,43,F,Administrator,68106\n90,60,M,Educator,78155\n91,55,M,marketing,01913\n92,32,M,Entertainment,80525\n93,48,M,Cheif Executive Officer,23112\n94,26,M,Student,71457\n95,31,M,Administrator,10707\n96,25,F,Artist,75206\n97,43,M,Artist,98006\n98,49,F,Cheif Executive Officer,90291\n99,20,M,Student,63129\n100,36,M,Cheif Executive Officer,90254\n101,15,M,Student,05146\n102,38,M,Programmer,30220\n103,26,M,Student,55108\n104,27,M,Student,55108\n105,24,M,Engineer,94043\n106,61,M,retired,55125\n107,39,M,Scientist,60466\n108,44,M,Educator,63130\n109,29,M,Other,55423\n110,19,M,Student,77840\n111,57,M,Engineer,90630\n112,30,M,salesman,60613\n113,47,M,Cheif Executive Officer,95032\n114,27,M,Programmer,75013\n115,31,M,Engineer,17110\n116,40,M,healthcare,97232\n117,20,M,Student,16125\n118,21,M,Administrator,90210\n119,32,M,Programmer,67401\n120,47,F,Other,06260\n121,54,M,Librarian,99603\n122,32,F,Writer,22206\n123,48,F,Artist,20008\n124,34,M,Student,60615\n125,30,M,Lawyer,22202\n126,28,F,Lawyer,20015\n127,33,M,cheif Cheif Executive Officer Officer,73439\n128,24,F,marketing,20009\n129,36,F,marketing,07039\n130,20,M,cheif Cheif Executive Officer Officer,60115\n131,59,F,Administrator,15237\n132,24,M,Other,94612\n133,53,M,Engineer,78602\n134,31,M,Programmer,80236\n135,23,M,Student,38401\n136,51,M,Other,97365\n137,50,M,Educator,84408\n138,46,M,doctor,53211\n139,20,M,Student,08904\n140,30,F,Student,32250\n141,49,M,Programmer,36117\n142,13,M,Other,48118\n143,42,M,Technician,08832\n144,53,M,Programmer,20910\n145,31,M,Entertainment,20910\n146,45,M,Artist,83814\n147,40,F,Librarian,02143\n148,33,M,Engineer,97006\n149,35,F,marketing,17325\n150,20,F,Artist,02139\n151,38,F,Administrator,48103\n152,33,F,Educator,68767\n153,25,M,Student,60641\n154,25,M,Student,53703\n155,32,F,Other,11217\n156,25,M,Educator,08360\n157,57,M,Engineer,70808\n158,50,M,Educator,27606\n159,23,F,Student,55346\n160,27,M,Programmer,66215\n161,50,M,Lawyer,55104\n162,25,M,Artist,15610\n163,49,M,Administrator,97212\n164,47,M,healthcare,80123\n165,20,F,Other,53715\n166,47,M,Educator,55113\n167,37,M,Other,55113\n168,48,M,Other,80127\n169,52,F,Other,53705\n170,53,F,healthcare,30067\n171,48,F,Educator,78750\n172,55,M,marketing,22207\n173,56,M,Other,22306\n174,30,F,Administrator,52302\n175,26,F,Scientist,21911\n176,28,M,Scientist,07030\n177,20,M,Programmer,19104\n178,26,M,Other,49512\n179,15,M,Entertainment,20755\n180,22,F,Administrator,60202\n181,26,M,Cheif Executive Officer,21218\n182,36,M,Programmer,33884\n183,33,M,Scientist,27708\n184,37,M,Librarian,76013\n185,53,F,Librarian,97403\n186,39,F,Cheif Executive Officer,00000\n187,26,M,Educator,16801\n188,42,M,Student,29440\n189,32,M,Artist,95014\n190,30,M,Administrator,95938\n191,33,M,Administrator,95161\n192,42,M,Educator,90840\n193,29,M,Student,49931\n194,38,M,Administrator,02154\n195,42,M,Scientist,93555\n196,49,M,Writer,55105\n197,55,M,Technician,75094\n198,21,F,Student,55414\n199,30,M,Writer,17604\n200,40,M,Programmer,93402\n201,27,M,Writer,93402\n202,41,F,Educator,60201\n203,25,F,Student,32301\n204,52,F,Librarian,10960\n205,47,M,Lawyer,06371\n206,14,F,Student,53115\n207,39,M,marketing,92037\n208,43,M,Engineer,01720\n209,33,F,Educator,85710\n210,39,M,Engineer,03060\n211,66,M,salesman,32605\n212,49,F,Educator,61401\n213,33,M,Cheif Executive Officer,55345\n214,26,F,Librarian,11231\n215,35,M,Programmer,63033\n216,22,M,Engineer,02215\n217,22,M,Other,11727\n218,37,M,Administrator,06513\n219,32,M,Programmer,43212\n220,30,M,Librarian,78205\n221,19,M,Student,20685\n222,29,M,Programmer,27502\n223,19,F,Student,47906\n224,31,F,Educator,43512\n225,51,F,Administrator,58202\n226,28,M,Student,92103\n227,46,M,Cheif Executive Officer,60659\n228,21,F,Student,22003\n229,29,F,Librarian,22903\n230,28,F,Student,14476\n231,48,M,Librarian,01080\n232,45,M,Scientist,99709\n233,38,M,Engineer,98682\n234,60,M,retired,94702\n235,37,M,Educator,22973\n236,44,F,Writer,53214\n237,49,M,Administrator,63146\n238,42,F,Administrator,44124\n239,39,M,Artist,95628\n240,23,F,Educator,20784\n241,26,F,Student,20001\n242,33,M,Educator,31404\n243,33,M,Educator,60201\n244,28,M,Technician,100005\n245,26,F,Technician,110000\n"
     ]
    }
   ],
   "source": [
    "for record in users_rdd.collect():\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc735bbc-8f18-4d6e-9520-e6be3d0dbfc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3648d5bd-dfe4-470d-8fad-056296e533be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: 20"
     ]
    }
   ],
   "source": [
    "rdd_02.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "485d252b-697b-40cb-b563-3514767576d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01 Exploring Spark Core API - RDD's",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
